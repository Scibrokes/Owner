---
title: "RYO®, ENG LIAN HU"
output:
  html_document:
    fig_height: 3
    fig_width: 5
    highlight: haddock
    theme: cerulean
    toc: yes
  pdf_document:
    fig_height: 3
    fig_width: 5
    highlight: haddock
    keep_tex: yes
    toc: yes
  word_document:
    fig_height: 3
    fig_width: 5
    highlight: haddock
---

```{r load-packages, include=FALSE}
# Don't delete this chunk if you are using the mosaic package
# This loads the mosaic and dplyr packages

## Setting to omit all warnings
options(warn=-1)

## Loading the packages
if(!'devtools' %in% installed.packages()){
  install.packages('devtools')}
if(!'BBmisc' %in% installed.packages()){
  install.packages('BBmisc')}
if(!'BiocParallel' %in% installed.packages()){
  devtools::install_github('Bioconductor/BiocParallel')}

suppressPackageStartupMessages(library('BBmisc'))
pkgs <- c('mosaic','plyr','dplyr','devtools','zoo','lubridate','stringr','rvest','googleVis','knitr','markdown','rmarkdown')
suppressAll(lib(pkgs)); rm(pkgs)
```

```{r setting-01, include=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).

# This changes the default colors in lattice plots.
trellis.par.set(theme=theme.mosaic())  

# knitr settings to control how R chunks work.
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small"    # slightly smaller font for code
)
# This loads the mosaic data sets.  (Could be deleted if you are not using them.)
# require('mosaicData')

# Set the googleVis options first to change the behaviour of plot.gvis, so that only the chart component of the HTML file is written into the output file.
op <- options(gvis.plot.tag='chart')
```




## Curriculum Vitae

<img src='figure/my-passport-size-photo.jpg' width='120'>

$®γθ$, Eng Lian Hu




### About Me

| **Title**                  |                                                                                          **Details** |
|:---------------------------|-----------------------------------------------------------------------------------------------------:|
| Date of Birth              |                                                                            `r x='22-10-1984';dmy(x)` |
| Age                        |      `r new_interval(dmy(x),now()) %>% as.period(,.unit=years) %>% str_replace(.,'(?<=d).{1,}$','')` |
| Malaysian Phone Number     |                                                                   [+6-017-2100905](tel:+60172100905) |
| Taiwanese Phone Numner     |                                                              [+886-098-910-4576](tel:+8860989104576) |
| Home Landline Contact      |                                                                   [+6-03-89570938](tel:+60389570938) |
| Email Address              |                                                                                <englianhu@gmail.com> |
| Nationality                |                                                                                             Japanese |
| Religion                   |                                                                                               Buddha |



### Education

Apr-2014 to May-2016 (expected)
:   **Professional Certificate, Specialization Data Science**; [Cousera](http://www.coursera.org) - [Johns Hopkins University (JHU)](https://www.jhu.edu/)
    
    * *01 Course title : [The Data Scientist's Toolbox](https://www.coursera.org/account/accomplishments/records/AjMdkzyHyA2JWRRL)*
    * *02 Course title : [R Programming](https://www.coursera.org/account/accomplishments/certificate/VBB2XUA29B)*
    * *03 Course title : [Getting and Cleaning Data](https://www.coursera.org/account/accomplishments/records/Q5HBLbpBekrW43Jd)*
    * *04 Course title : [Exploratory Data Analysis](https://www.coursera.org/account/accomplishments/records/4Gz8BmPsnuknW93A)*
    * *05 Course title : [Reproducible Research](https://www.coursera.org/account/accomplishments/records/xpNfvxWs8uMMYrjs)*
    * *06 Course title : [Statistical Inference](https://www.coursera.org/account/accomplishments/records/jXzs4bAFXZfJ3Dne)*
    * *07 Course title : [Regression Models](https://www.coursera.org/account/accomplishments/records/Su7PgrMXVGYrU4cM)*
    * *08 Course title : [Practical Machine Learning](https://www.coursera.org/account/accomplishments/records/j6DpQhLYeRj54dvS)*
    * *09 Course title : [Developing Data Products](https://www.coursera.org/account/accomplishments/records/ERvuyAZm6CG45EUS)*
    * *10 Course title : [Data Science Capstone]()*
    
Jun-2015 to Aug-2015
:   **Professional Certificate, Specialization Data Science**; [Cousera](http://www.coursera.org) - [University of Michigan (UM)](https://www.umich.edu/)
    
    * *Course title : [Programming for Everybody (Python)](https://www.coursera.org/account/accomplishments/records/GA4cGW59ETHAevCa)*
    
Oct-1984 to Present
:   [**日本史**](http://www.t3.rim.or.jp/~miukun/); 明治学校
    
    * [**第一話** *はじめに*](http://www.t3.rim.or.jp/~miukun/japan1.htm)
    * [**第二話** *日本人の起源*](http://www.t3.rim.or.jp/~miukun/japan2.htm)
    * [**第三話** *古代王朝*](http://www.t3.rim.or.jp/~miukun/japan3.htm)
    * [**第四話** *平安時代*](http://www.t3.rim.or.jp/~miukun/japan4.htm)
    * [**第五話** *幕府政治の登場*](http://www.t3.rim.or.jp/~miukun/japan5.htm)
    * [**第六話** *南北朝と室町*](http://www.t3.rim.or.jp/~miukun/japan6.htm)
    * [**第七話** *革新の時代*](http://www.t3.rim.or.jp/~miukun/japan7.htm)
    * [**第八話** *徳川幕府*](http://www.t3.rim.or.jp/~miukun/japan8.htm)
    * [**第九話** *明治維新*](http://www.t3.rim.or.jp/~miukun/japan9.htm)
    * [**第十話** *帝国主義の道*](http://www.t3.rim.or.jp/~miukun/japan10.htm)
    * [**第十一話** *そして、今*](http://www.t3.rim.or.jp/~miukun/japan11.htm)
    
Oct-1984 to Present
:   [**中国史**](http://www.uraken.net/rekishi/rekichina.html); 明治学校
    
    * [**第１回** *古代中国～周、そして秦へ*](http://www.uraken.net/rekishi/reki-chu01.html)
    * [**第２回** *秦の始皇帝の時代*](http://www.uraken.net/rekishi/reki-chu02.html)
    * [**第３回** *漢の時代*](http://www.uraken.net/rekishi/reki-chu03.html)
    * [**第４回** *後漢～復活した漢だが～*](http://www.uraken.net/rekishi/reki-chu04.html)
    * [**第５回** *群雄割拠・三国志時代*](http://www.uraken.net/rekishi/reki-chu05.html)
    * [**第６回** *南北朝時代*](http://www.uraken.net/rekishi/reki-chu06.html)
    * [**第７回** *華開く南北朝文化*](http://www.uraken.net/rekishi/reki-chu07.html)
    * [**第８回** *中華再統一した隋*](http://www.uraken.net/rekishi/reki-chu08.html)
    * [**第９回** *女性皇帝も出た唐の時代*](http://www.uraken.net/rekishi/reki-chu09.html)
    * [**第１０回** *国際色豊かな唐の文化*](http://www.uraken.net/rekishi/reki-chu010.html)
    * [**第１１回** *五代十国から宋へ*](http://www.uraken.net/rekishi/reki-chu011.html)
    * [**第１２回** *華北の金と江南の南宋*](http://www.uraken.net/rekishi/reki-chu012.html)
    * [**第１３回** *宋の文化*](http://www.uraken.net/rekishi/reki-chu013.html)
    * [**第１４回** *モンゴル帝国*](http://www.uraken.net/rekishi/reki-chu014.html)
    * [**第１５回** *元の中国支配*](http://www.uraken.net/rekishi/reki-chu015.html)
    * [**第１６回** *モンゴル・元代の文化*](http://www.uraken.net/rekishi/reki-chu016.html)
    * [**第１７回** *最後の漢民族王朝・明の建国*](http://www.uraken.net/rekishi/reki-chu017.html)
    * [**第１８回** *明時代の東シナ海*](http://www.uraken.net/rekishi/reki-chu018.html)
    * [**第１９回** *北虜南倭の患、明の衰退*](http://www.uraken.net/rekishi/reki-chu019.html)
    * [**第２０回** *百科事典も登場、明の文化*](http://www.uraken.net/rekishi/reki-chu020.html)
    * [**第２１回** *女真族の後金・清と李自成の乱*](http://www.uraken.net/rekishi/reki-chu021.html)
    * [**第２２回** *清の草創期～ドルゴンと順治帝～*](http://www.uraken.net/rekishi/reki-chu022.html)
    * [**第２３回** *名君の時代１～康煕帝～*](http://www.uraken.net/rekishi/reki-chu023.html)
    * [**第２４回** *名君の時代２～雍正帝～*](http://www.uraken.net/rekishi/reki-chu024.html)
    * [**第２５回** *名君の時代３～乾隆帝～*](http://www.uraken.net/rekishi/reki-chu025.html)
    * [**第２６回** *アヘン戦争前夜*](http://www.uraken.net/rekishi/reki-chu026.html)
    * [**第２７回** *アヘン戦争*](http://www.uraken.net/rekishi/reki-chu027.html)
    * [**第２８回** *今度はキリスト教？太平天国の乱*](http://www.uraken.net/rekishi/reki-chu028.html)
    * [**第２９回** *第２次アヘン戦争のアロー戦争*](http://www.uraken.net/rekishi/reki-chu029.html)
    * [**第３０回** *洋務運動と日仏露との領土問題*](http://www.uraken.net/rekishi/reki-chu030.html)
    * [**第３１回** *日清戦争*](http://www.uraken.net/rekishi/reki-chu031.html)
    
    
    
### Experience
    
May-2015 to Sep-2015
:   **Online Gaming, Customer Service**; OneMedia Sdn Bhd
    
    * *Salary : MYR2500*
    * *Job Task : Handling inbound and outbound calls, livechat, feedback, all deposit and withdrawal transactions.*
    * *Resson of Leaving : Superior request change to commission based sales and marketing but not my strength, therefore choose resign.*
    
Feb-2015 to Apr-2015
:   **Online Retails, Fraud Prevention**; [Apple Inc.](http://www.apple.com)
    
    * *Salary : MYR4000*
    * *Job Task : Handling inbound and outbound calls, check and approve/reject customers' ordered transactions.*
    * *Resson of Leaving : Superior request to hit certain quantity of filtering transactions, therefore choose resign.*
    
Nov-2013 to Apr-2014
:   **Online Gaming Trading, Team Leader/Trader**; [SBSolutionCorp Ltd](http://www.gb-links.com/)
    
    * *Salary : MYR3800*
    * *Job Task : Handling few members to update scores and bets settlement.*
    * *Resson of Leaving : Superior request hire more team members to lease the job, but I just would like to enjoy simple life office worker, therefore choose resign.*
    
Nov-2012 to May-2013
:   **TV Broadcasting Channel, Video Editor**; [U-Drive Media Sdn Bhd](http://www.udrive-media.com/)
    
    * *Salary : MYR2450*
    * *Job Task : Watching and filtering video streaming from abroad prior to play in our country.*
    * *Resson of Leaving : Director request change to account or marketing department, but marketing is not my strength and I just would like to enjoy simple life office worker, therefore choose resign.*
    
Mar-2008 to Jul-2012
:   **Outsourcing Business, Customer Service**; [Scicom (MSC) Bhd](http://www.scicom-intl.com/)
    
    * *Salary : `r paste0('MYR',3250 * 1.125)`*
    * *Job Task : Handling inbound and outbound calls, livechat, feedback, all deposit, withdrawal and also phone bets' transactions. Sometimes need to do translation and also analysis job tasks*
    * *Resson of Leaving : HR request change project since closed project, since there has no sportsbook related project besides Ladbrokes during that time therefore choose resign.*
    
Jul-2006 to Jun-2007
:   **Online Gambling, Trainer/Trader**; Caspo Inc.
    
    * *Salary : MYR`r 5000 + 220 * read_html('http://www.bloomberg.com/quote/USDMYR:CUR') %>% html_nodes(xpath='//*[@id="content"]/div/div/div[1]/div/div[5]/div[2]') %>% html_text %>% as.numeric`*
    * *Job Task : Provides training to newbies on sportsbook trading and live scout. Need to backup for trading as well. Sometimes need to do translation and also analysis job tasks.*
    * *Resson of Leaving : Cannot stand for daily 12 to 16 working hours during that time, therefore choose resign.*
    
Nov-2005 to Jul-2006
:   **Online Gambling, Team Leader/Trader**; [Telebiz Sdn Bhd](http://www.telebizness.com/).
    
    * *Salary : MYR2450*
    * *Job Task : Provides training to newbies on sportsbook trading purchasing stocks, need to update scores for bets financial settlement. Handle live-matches on corners. Sometimes need to do translation and also analysis job tasks.*
    * *Resson of Leaving : Looking for better offer which introduced by friends.*
    
    
    
### Technical Experience
    
Jul-2015 to Present
:   [**Betting Strategy and Model Validation**](https://github.com/Scibrokes/Betting-Strategy-and-Model-Validation)
    
    * *Learn PhantomJS for background wbdriver, markdown, setup RStudio server. However RSelenium and PhamtonJS faced some error and then just simply apply rvest to harvest the data from spbo without odds price.*
    * *Gather and filtering the staking dataset from Sportsbook consultancy firm A and validate their staking models.*
    * *Gather livescore data from* [*spbo*](http://www.spbo.com/eend0.htm) *livescore website and filter the data.*
    * *Natural language analysis on the scrapped livescore data to match the team names from firm A.*
    * *Testing the efficiency of some coding.*
    
    
Dec-2014 to Dec-2014
:   [**Testing Inefficiency of Sports-Bookmakers by Kelly Model**](https://github.com/Scibrokes/Kelly-Criterion)
    
    * *Apply poisson model to the scrapped data from* **WebDriver Dynamic Webpage Scrapping** *to build an prediction model.*
    * *Rewrite the Kelly model from* **Odds Modelling and Testing Inefficiency of Sports-Bookmakers** *in R but also optimal value* $r$ *which is stated in* [*Dixon&Coles1996*](http://www.math.ku.dk/~rolf/teaching/thesis/DixonColes.pdf) *to test the efficiency and the returns of investment based from the Poisson model.*
    * *Improve the efficiency of data management and also timing of calculation.*
    
    
Feb-2014 to Mar-2014
:   [**Dixon-Coles1996**](https://github.com/Scibrokes/Dixon-Coles1996)
    
    * *Learning* `Shiny` *and* `knitr` *in Coursera JHU* **09 Developing Data Products**, *occasionally noticed a* `fbRanks` *package, simply scrape the livescore of English Premier League to test the Dixon-Coles model.*
    * *Similar with my previous research, the author wrote a scrape the data statical webpage but save as csv, but add-on a surface/venue parameter and also apply efficiency of the calculation for bigger size dataset. You can go to* <http://www.lastplanetranking.blogspot.my> *for more details.*
    * *I've modified a bit my model by refer the idea* [efficiency of glm packages](http://stackoverflow.com/questions/19532651/benchmarking-logistic-regression-using-glm-fit-bigglm-speedglm-glmnet-libli) *of the author and will review again my model and also write my company website* [Scibrokes® Test Site](http://www.scibrokes.com/Test/) *by Shiny when free.*
    * *Additional information about the website server*, [SoccerMetrics](http://www.soccermetrics.net/), [sotdoc](http://www.sotdoc.co.uk/) *and* [matchOdds.org](http://www.matchodds.org) *might be good referencens for my future writing website. SotDoc roughly brief about the annual returns of* [SmartOdds](http://www.smartodds.co.uk/) *around 2.5~3% while their investment returns rate around 7% per annum comapre to most of sportsbookmakers 0.5% or Crown 1.8%. SoccerMetrics using Python to setup* [Soccermetrics API Python Client](https://github.com/englianhu/soccermetrics-client-py) *while MatchOdds apply Tomcat to manage the data server.*
    
    
Mar-2014 to Apr-2014
:   [**WebDriver Dynamic Webpage Scrapping**](https://github.com/Scibrokes/WebDriver-DynamicWebpage-Scrapping)
    
    * *Relenium and RSelenium published, scrape the odds price of sportbookmakers and also livescore data from* [*7M*](http://odds.7m.hk/en/) *and* [*NowGoal*](http://info.nowgoal.com/en/) *website as well as filter the odds price data.*
    
    
Aug-2013 to Sep-2013
:   [**Soccer League Web Scraping**](https://github.com/Scibrokes/Soccer-League-Web-Scraping)
    
    * *Learning programming to gather livescore and also odds price data from* [*Gooooal*](http://app.en.gooooal.com/soccer/statistic/standing.do). *livescore website.*
    * *From the scrapped result, there has alot of errors and wrong result since the static website result unable changed once the score is updated. There might be some postponed matches and also suspended metches. Moreover dynamic webpage request selenium apps, due to Rpy loss maintenance and rPython,RSelenium are not yet published. Therefore forfeit and learn Python and started the* **WebDriver Dynamic Webpage Scrapping**.
    
    
Apr-2008 to Apr-2010
:   [**Odds Modelling and Testing Inefficiency of Sports-Bookmakers**](https://www.dropbox.com/sh/ifwczokjptt6re0/AADv1VarJoQ6IgIitZBzG5c6a?dl=0)
    
    * *Learn RExcel, CrystalBall, ModelRisk etc. and choose R open source software to start my research.*
    * *Collect the livescore and also 1x2, Asian Handicap, Over Under odds price data of 29 sportsbookmakers manually from* [*500WAN*](http://www.500wan.com), [*BET007*](http://www.bet007.com) *and* [*NowGoal*](http://info.nowgoal.com/en/) *website and filter the odds price data from 2006 to 2011.*
    * *Apply Poisson model in R to test the return of the investment. This research job is the most completed, success and the first research which write the whole odds compilation EM model and data management by refer to thousands of research papers in sportsbook odds modelling after resigned from Caspo Inc.*
    * *Will write an proper thesis during my spared time.*
    
    
Jan-2007 to Jul-2007
:   [**Apply Poisson regression on sports odds modelling**](https://www.dropbox.com/sh/h3e6gv59onz311j/AAAFcfFOfon-kEhveTzvHGHza?dl=0)
    
    * *Write own Poisson model based on what I learnt from* [*Stuart Doyle*](https://www.facebook.com/stuart.doyle1) *during working in Telebiz Sdn Bhd.*
    * *Due to not master in Macro VBA, I just simply recall the concept which was a spreadsheet VBA Excel file from* [*Paul Judge*](https://www.facebook.com/paul.judge.14) *for data management I accidentlly deleted few years ago.*
    * *Remarks: Stuart and Paul used be Sportsbook consultants to Telebiz and setup own advisor company [SportsTrust](http://www.sportstrust.com) during 2009/2010.*



### Time Line

```{r timeline, echo=FALSE, results='asis'}
tmline <- data.frame(YearMonth=as.yearmon(c('2003-12','2004-06','2005-10','2005-11','2005-12','2006-03','2006-06','2006-07','2006-07','2006-08','2006-10','2007-07','2008-04','2010-04','2010-04','2013-09','2013-10','2014-03','2014-04','2014-12','2015-08')),Event=c('JLPT Level3','Joined Forex Trading Line','Joined Sportsbook Line','Learn Korean Language','Transfered to Purchasing Deprt','Learn Poisson Model','Conduct Corners BIR','Resigned from Telebiz','Join Caspo Inc','Transfered to Training Deprt','Learn Deutsch Language','Write own Basic Poisson Model','Joined Scicom (MSC) Bhd','Learn Sportsbook Line and Odds Modelling thoroughly','Odds Modelling and Testing Inefficiency of Sportsbookmakers','Soccer League Web Scrapping','Registered Scibrokes Trading Enterprise','Test Dixon-Coles1996 model','WebDriver Dynamic Webpage Scrapping','Testing Inefficiency of Sports-Bookmakers by Kelly Model','Professional Certificate - Specialization Data Science (Python)'))
tmline <- data.frame(No=seq(nrow(tmline)),tmline)
tmline$YearMonth <- as.Date(paste0("01",tmline$YearMonth), format="%d%b%Y")
#'@ write.csv(tmline,file=paste0(getwd(),'/dataset/dfm.csv'))
#'@ tmline <- read.csv(file=paste0(getwd(),'/dataset/dfm.csv')) %>% tbl_df

G <- gvisTable(tmline, options=list(width='automatic', height=400))
M <- gvisMotionChart(tmline, idvar='Event', date.format='%b-%Y', timevar='YearMonth', options=list(width='automatic', height=400))
GM <- gvisMerge(G, M, horizontal=TRUE, tableOptions='bgcolor=\'#CCCCCC\' cellspacing=10')
rm(G, M)
plot(GM)
#'@ print(GM,'chart')
```

```{r setting-02, include=FALSE}
## Set options back to original options
options(op)
```



### Skill

```{r skill, echo=FALSE, result='asis'}
skill <- data.frame(c('R Programming','MS Office','SQL','Python','Data Analysis','Customer Service','Sportsbook Industry'),c(7,8,3,4,9,9,7)) %>% tbl_df
names(skill) <- c('Skill','Level (from newbie 1 to expert 10)')
kable(skill)
```



### Other/Miscellaneous

I setup [Scibrokes Test Site](http://www.scibrokes.com/Test/) and also [Scibrokes RStudio Server](http://rstudio.scibrokes.com) <img src='figure/Scibrokes®-TonyStark®.jpg' width='24'> to manage data mining, data analysis and also calculation. You are welcome to own yours by refer to my blog --- [Own a RStudio Shiny Server](https://englianhu.wordpress.com/statistics/own-a-rstudio-shiny-server/). You are welcome to contact me at <englianhu@gmail.com> for any business coorperation since I seldom check my company mailboxes <mofa@scibrokes.com>, <contact@scibrokes.com>, <ryo@scibrokes.com>.



### Social Network

<a href='https://www.coursera.org/user/i/617ad9f360f8fa19031b59c9af23b799'><img src='figure/coursera_36x36.jpg' height='48'></a>
<a href='https://jp.linkedin.com/in/englianhu'><img src='figure/linkedin_36x36.jpg' height='48'></a>
<a href='skype:englianhu?call'><img src='figure/skype_36x36.jpg' height='48'></a>
<a href='https://www.facebook.com/englianhu'><img src='figure/facebook2_36x36.jpg' height='48'></a>
<a href='https://plus.google.com/+黄联富'><img src='figure/gplus_36x36.jpg' height='48'></a>
<a href='https://twitter.com/englianhu'><img src='figure/twitter_36x36.jpg' height='48'></a>
<a href='https://about.me/englianhu'><img src='figure/aboutme_36x36.jpg' height='48'></a>
<a href='https://instagram.com/englianhu/'><img src='figure/instagram_36x36.jpg' height='48'></a>
<a href='http://www.weibo.com/englianhu'><img src='figure/weibo_36x36.jpg' height='48'></a>
<a href='http://t.qq.com/englianhu'><img src='figure/QQ wb_36x36.jpg' height='48'></a>
<a href='http://user.qzone.qq.com/2837330579'><img src='figure/Qzone_36x36.jpg' height='48'></a>
<a href='https://scibrokes.zeef.com/ryo.eng'><img src='figure/zeef_36x36.jpg' height='48'></a>
<a href='https://disqus.com/by/englianhu'><img src='figure/disqus_36x36.jpg' height='48'></a>
<a href='https://github.com/englianhu'><img src='figure/github_36x36.jpg' height='48'></a>
<a href='https://bitbucket.org/englianhu/'><img src='figure/bitbucket_36x36.jpg' height='48'></a>
<a href='https://plot.ly/~englianhu'><img src='figure/plotly_36x36.jpg' height='48'></a>
<a href='http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=user_nodes&user=379689'><img src='figure/rnable_36x36.jpg' height='48'></a>
<a href='http://www.talkstats.com/member.php/107100-englianhu'><img src='figure/talkstats_36x36.jpg' height='48'></a>
<a href='http://blog.csdn.net/github_25731367'><img src='figure/csdn_36x36.jpg' height='48'></a>
<a href='http://www.linuxquestions.org/questions/user/englianhu-1019212/'><img src='figure/LQ_36x36.jpg' height='48'></a>
<a href='http://www.biostatistic.net/?120034'><img src='figure/biostatistic_36x36.jpg' height='48'></a>
<a href='http://cos.name/cn/profile/ryo/'><img src='figure/cos_36x36.jpg' height='48'></a>
<a href='http://bbs.pinggu.org/home.php?mod=space&uid=5794471'><img src='figure/jg_36x36.jpg' height='48'></a>
<a href='http://mathchina.net/dvbbs/show.asp?username=englianhu'><img src='figure/dwang_36x36.jpg' height='48'></a>
<a href='http://www.mathchina.com/bbs/home.php?mod=space&uid=44479'><img src='figure/mathchina_36x36.jpg' height='48'></a>
<a href='https://www.centos.org/forums/search.php?author_id=100465&sr=posts'><img src='figure/centOS_36x36.jpg' height='48'></a>
<a href='http://www.betting-forum.com/members/englianhu.10020/'><img src='figure/bettingforum_36x36.jpg' height='48'></a>
<a href='http://https://forum.punterslounge.com/profile/94378-ryoeng/'><img src='figure/PLounge_36x36.jpg' height='48'></a>
<a href='http://www.sportsbookreview.com/forum/'><img src='figure/SBR_36x36.jpg' height='48'></a>




## Appendices
  
  
  
### Documenting File Creation 

It's useful to record some information about how your file was created.

  * File created/updated date: `r Sys.Date()`
  * `r R.version.string`
  * R version (short form): `r getRversion()`
  * `mosaic` package version: `r packageVersion("mosaic")`
  * Github: [source code](https://github.com/Scibrokes/Owner)
  * Additional session information
  
```{r echo=FALSE, results='asis'}
session_info()$platform
```



### References

  * You are feel free to create  your own CV by R Markdown V2 by refer to [Document Templates](http://rmarkdown.rstudio.com/developer_document_templates.html?version=0.99.484&mode=server)
  * `rticles` package version: `r packageVersion("rticles")`
  * [Pandoc Markdown](http://rmarkdown.rstudio.com/authoring_pandoc_markdown.html)
  * You can plot a [timeline](http://stackoverflow.com/questions/20695311/chronological-timeline-with-points-in-time-and-format-date) graph or load the [timeline](http://jason.bryer.org/timeline/) package.
  * Here I apply [googleVis](https://cran.r-project.org/web/packages/googleVis/vignettes/Using_googleVis_with_knitr.html) package to plot the animated graph. You are feel free to browse over the [vignettes](https://cran.r-project.org/web/packages/googleVis/vignettes/googleVis.pdf) for more examples.
  * You can also modify the layout of website by refering [Top Five CSS Customizations for R Presentations](http://rstudio-pubs-static.s3.amazonaws.com/27777_55697c3a476640caa0ad2099fe914ae5.html#/).
  
  
  